library(MLmetrics)
MLmetrics::RMSLE(y_pred = predicted_points_1$SalePrice ,y_true predicted_points_1$.fitted)
library(MLmetrics)
MLmetrics::RMSLE(y_pred = predicted_points_1$SalePrice ,y_true predicted_points_1$.fitted)
MLmetrics::RMSLE(y_pred = predicted_points_1$SalePrice ,y_true =  predicted_points_1$.fitted)
# Chunk 1: setup
# Load all your packages here:
library(tidyverse)
library(scales)
library(Mlmetrics)
library(broom)
# Set default behavior for all code chunks here:
knitr::opts_chunk$set(
echo = TRUE, warning = FALSE, message = FALSE,
fig.width = 16/2, fig.height = 9/2
)
# Set seed value of random number generator here. This is in order to get
# "replicable" randomness, so that any results based on random sampling or
# resampling are replicable everytime you knit this file. Why use a seed value
# of 76? For no other reason than 76 is one of my favorite numbers:
# https://www.youtube.com/watch?v=xjJ7FheCkCU
set.seed(76)
# Chunk 2
training <- read_csv("data/train.csv")
test <- read_csv("data/test.csv")
sample_submission <- read_csv("data/sample_submission.csv")
# Chunk 4
glimpse(sample_submission)
# Chunk 5
#variables:   Categorical - BldgType,  Numerical - GrLivArea
# training <- training %>%
#   select(Id, GrLivArea, BldgType, SalePrice)
# test <- test %>%
#   select(Id, GrLivArea, BldgType)
# Chunk 6
fake_train <- training %>%
sample_frac(0.75)
fake_test <- training %>%
anti_join(fake_train, by="Id")
# Chunk 7
# 1. Fit model to training data
model_1_formula <- as.formula("SalePrice~ GrLivArea + BldgType")
model_1 <- lm(model_1_formula, data = fake_train)
# 2.a) Extract regression table with confidence intervals
model_1 %>%
broom::tidy(conf.int = TRUE)
# 2.b) Extract point-by-point info of points used to fit model
fitted_points_1 <- model_1 %>%
broom::augment()
fitted_points_1
# 2.c) Extract model summary info
model_1 %>%
broom::glance()
# 3. Make predictions on test data. Compare this to use of broom::augment()
# for fitted_points()
predicted_points_1 <- model_1 %>%
broom::augment(newdata = fake_test)
predicted_points_1
# Chunk 8
MLmetrics::RMSLE(y_pred = predicted_points_1$SalePrice ,y_true =  predicted_points_1$.fitted)
# Chunk 9
submission <- sample_submission %>%
mutate(SalePrice = mean(training$SalePrice))
write_csv(submission, path = "data/submission_mvp.csv")
# Chunk 10
# 1. Fit model to training data
model_2_formula <- as.formula("SalePrice~ GrLivArea + BldgType + OverallQual + HalfBath + Foundation + SaleCondition")
model_2 <- lm(model_2_formula, data = fake_train)
# 2.a) Extract regression table with confidence intervals
model_2 %>%
broom::tidy(conf.int = TRUE)
# 2.b) Extract point-by-point info of points used to fit model
fitted_points_2 <- model_2 %>%
broom::augment()
fitted_points_2
# 2.c) Extract model summary info
model_2 %>%
broom::glance()
# 3. Make predictions on test data. Compare this to use of broom::augment()
# for fitted_points()
predicted_points_2 <- model_2 %>%
broom::augment(newdata = fake_test)
predicted_points_2
# HAVE TO LOG BEFORE FITTING AND UNLOG AFTER PREDICTING
# Chunk 11
MLmetrics::RMSE(y_pred = predicted_points_2$SalePrice, y_true = predicted_points_2$.fitted)
# Chunk 12
submission <- sample_submission %>%
mutate(SalePrice = mean(training$SalePrice))
write_csv(submission, path = "data/submission_due_diligence.csv")
# Chunk 13
# Chunk 14
# Chunk 15
submission <- sample_submission %>%
mutate(SalePrice = mean(training$SalePrice))
write_csv(submission, path = "data/submission_reach_for_stars.csv")
# Chunk 16
# Chunk 17
# Chunk 18
submission <- sample_submission %>%
mutate(SalePrice = mean(training$SalePrice))
write_csv(submission, path = "data/submission_diminishing_returns.csv")
MLmetrics::RMSE(y_pred = predicted_points_2$SalePrice, y_true = predicted_points_2$.fitted)
MLmetrics::RMSLE(y_pred = predicted_points_2$SalePrice, y_true = predicted_points_2$.fitted)
View(fitted_points_2)
View(predicted_points_1)
training <- training %>%
mutate(log10_SalePrice = log10(SalePrice),
log10_GrLivArea = log10(GrLivArea),
log10_BldType = log10(BldgType),
log10_OverallQual = log10(OverallQual),
log10_HalfBath = log10(HalfBath),
log10_Foundation = log10(Foundation),
log10_SaleCondition = log10(SaleCondition)
)
str(test)
training <- training %>%
mutate(log10_SalePrice = log10(SalePrice),
log10_GrLivArea = log10(GrLivArea),
#log10_BldType = log10(BldgType),
log10_OverallQual = log10(OverallQual),
log10_HalfBath = log10(HalfBath),
#log10_Foundation = log10(Foundation),
#log10_SaleCondition = log10(SaleCondition)
)
training <- training %>%
mutate(log10_SalePrice = log10(SalePrice),
log10_GrLivArea = log10(GrLivArea),
#log10_BldType = log10(BldgType),
log10_OverallQual = log10(OverallQual),
log10_HalfBath = log10(HalfBath)
#log10_Foundation = log10(Foundation),
#log10_SaleCondition = log10(SaleCondition)
)
# 1. Fit model to training data
#model_2_formula <- as.formula("SalePrice~ GrLivArea + BldgType + OverallQual + HalfBath + Foundation + SaleCondition")
model_2_formula <- as.formula("log10_SalePrice~ log10_GrLivArea + log10_BldgType + log10_OverallQual + log10_HalfBath + log10_Foundation + log10_SaleCondition")
model_2 <- lm(model_2_formula, data = fake_train)
# 2.a) Extract regression table with confidence intervals
model_2 %>%
broom::tidy(conf.int = TRUE)
# 2.b) Extract point-by-point info of points used to fit model
fitted_points_2 <- model_2 %>%
broom::augment()
fitted_points_2
# 2.c) Extract model summary info
model_2 %>%
broom::glance()
# 3. Make predictions on test data. Compare this to use of broom::augment()
# for fitted_points()
predicted_points_2 <- model_2 %>%
broom::augment(newdata = fake_test)
predicted_points_2
# Save predictions in pretend_test data frame
fake_test <- fake_test %>%
mutate(
log10_SalePrice_hat = predicted_points_2$.fitted,
SalePrice_hat = 10^log10_SalePrice_hat
)
# HAVE TO LOG BEFORE FITTING AND UNLOG AFTER PREDICTING
MLmetrics::RMSLE(y_pred = fake_test$SalePrice_hat, y_true = fake_test$SalePrice)
MLmetrics::RMSLE(y_pred = fake_test$SalePrice_hat, y_true = fake_test$SalePrice)
View(fake_test)
# 1. Fit model to training data
#model_2_formula <- as.formula("SalePrice~ GrLivArea + BldgType + OverallQual + HalfBath + Foundation + SaleCondition")
model_2_formula <- as.formula("log10_SalePrice~ log10_GrLivArea + BldgType + log10_OverallQual + log10_HalfBath + Foundation + SaleCondition")
model_2 <- lm(model_2_formula, data = fake_train)
# 2.a) Extract regression table with confidence intervals
model_2 %>%
broom::tidy(conf.int = TRUE)
# 2.b) Extract point-by-point info of points used to fit model
fitted_points_2 <- model_2 %>%
broom::augment()
fitted_points_2
# 2.c) Extract model summary info
model_2 %>%
broom::glance()
# 3. Make predictions on test data. Compare this to use of broom::augment()
# for fitted_points()
predicted_points_2 <- model_2 %>%
broom::augment(newdata = fake_test)
predicted_points_2
# Save predictions in pretend_test data frame
fake_test <- fake_test %>%
mutate(
log10_SalePrice_hat = predicted_points_2$.fitted,
SalePrice_hat = 10^log10_SalePrice_hat
)
# HAVE TO LOG BEFORE FITTING AND UNLOG AFTER PREDICTING
MLmetrics::RMSLE(y_pred = fake_test$SalePrice_hat, y_true = fake_test$SalePrice)
View(fitted_points_2)
log10(307000)
training <- training %>%
mutate(log10_SalePrice = log10(SalePrice),
log10_GrLivArea = log10(GrLivArea),
#log10_BldType = log10(BldgType),
log10_OverallQual = log10(OverallQual),
log10_HalfBath = log10(HalfBath)
#log10_Foundation = log10(Foundation),
#log10_SaleCondition = log10(SaleCondition)
)
fake_train <- fake_train %>%
mutate(log10_SalePrice = log10(SalePrice),
log10_GrLivArea = log10(GrLivArea),
log10_OverallQual = log10(OverallQual),
log10_HalfBath = log10(HalfBath)
)
fake_test <- fake_test %>%
mutate(log10_SalePrice = log10(SalePrice),
log10_GrLivArea = log10(GrLivArea),
log10_OverallQual = log10(OverallQual),
log10_HalfBath = log10(HalfBath)
)
# 1. Fit model to training data
#model_2_formula <- as.formula("SalePrice~ GrLivArea + BldgType + OverallQual + HalfBath + Foundation + SaleCondition")
model_2_formula <- as.formula("log10_SalePrice~ log10_GrLivArea + BldgType + log10_OverallQual + log10_HalfBath + Foundation + SaleCondition")
model_2 <- lm(model_2_formula, data = fake_train)
# 2.a) Extract regression table with confidence intervals
model_2 %>%
broom::tidy(conf.int = TRUE)
# 2.b) Extract point-by-point info of points used to fit model
fitted_points_2 <- model_2 %>%
broom::augment()
fitted_points_2
# 2.c) Extract model summary info
model_2 %>%
broom::glance()
# 3. Make predictions on test data. Compare this to use of broom::augment()
# for fitted_points()
predicted_points_2 <- model_2 %>%
broom::augment(newdata = fake_test)
predicted_points_2
# Save predictions in pretend_test data frame
fake_test <- fake_test %>%
mutate(
log10_SalePrice_hat = predicted_points_2$.fitted,
SalePrice_hat = 10^log10_SalePrice_hat
)
# HAVE TO LOG BEFORE FITTING AND UNLOG AFTER PREDICTING
MLmetrics::RMSLE(y_pred = fake_test$SalePrice_hat, y_true = fake_test$SalePrice)
# Chunk 1: setup
# Load all your packages here:
library(tidyverse)
library(scales)
library(Mlmetrics)
library(broom)
# Set default behavior for all code chunks here:
knitr::opts_chunk$set(
echo = TRUE, warning = FALSE, message = FALSE,
fig.width = 16/2, fig.height = 9/2
)
# Set seed value of random number generator here. This is in order to get
# "replicable" randomness, so that any results based on random sampling or
# resampling are replicable everytime you knit this file. Why use a seed value
# of 76? For no other reason than 76 is one of my favorite numbers:
# https://www.youtube.com/watch?v=xjJ7FheCkCU
set.seed(76)
# Chunk 2
training <- read_csv("data/train.csv")
test <- read_csv("data/test.csv")
sample_submission <- read_csv("data/sample_submission.csv")
# Chunk 4
glimpse(sample_submission)
# Chunk 5
#variables:   Categorical - BldgType,  Numerical - GrLivArea
# training <- training %>%
#   select(Id, GrLivArea, BldgType, SalePrice)
# test <- test %>%
#   select(Id, GrLivArea, BldgType)
# Chunk 6
fake_train <- training %>%
sample_frac(0.75)
fake_test <- training %>%
anti_join(fake_train, by="Id")
# Chunk 7
# 1. Fit model to training data
model_1_formula <- as.formula("SalePrice~ GrLivArea + BldgType")
model_1 <- lm(model_1_formula, data = fake_train)
# 2.a) Extract regression table with confidence intervals
model_1 %>%
broom::tidy(conf.int = TRUE)
# 2.b) Extract point-by-point info of points used to fit model
fitted_points_1 <- model_1 %>%
broom::augment()
fitted_points_1
# 2.c) Extract model summary info
model_1 %>%
broom::glance()
# 3. Make predictions on test data. Compare this to use of broom::augment()
# for fitted_points()
predicted_points_1 <- model_1 %>%
broom::augment(newdata = fake_test)
predicted_points_1
# Chunk 8
MLmetrics::RMSLE(y_pred = predicted_points_1$SalePrice ,y_true =  predicted_points_1$.fitted)
# Chunk 9
submission <- sample_submission %>%
mutate(SalePrice = mean(training$SalePrice))
write_csv(submission, path = "data/submission_mvp.csv")
# Chunk 10
fake_train <- fake_train %>%
mutate(log10_SalePrice = log10(SalePrice),
log10_GrLivArea = log10(GrLivArea),
log10_OverallQual = log10(OverallQual),
log10_HalfBath = log10(HalfBath)
)
fake_test <- fake_test %>%
mutate(log10_SalePrice = log10(SalePrice),
log10_GrLivArea = log10(GrLivArea),
log10_OverallQual = log10(OverallQual),
log10_HalfBath = log10(HalfBath)
)
# Chunk 11
# 1. Fit model to training data
#model_2_formula <- as.formula("SalePrice~ GrLivArea + BldgType + OverallQual + HalfBath + Foundation + SaleCondition")
model_2_formula <- as.formula("log10_SalePrice~ log10_GrLivArea + BldgType + log10_OverallQual + log10_HalfBath + Foundation + SaleCondition")
model_2 <- lm(model_2_formula, data = fake_train)
# 2.a) Extract regression table with confidence intervals
model_2 %>%
broom::tidy(conf.int = TRUE)
# 2.b) Extract point-by-point info of points used to fit model
fitted_points_2 <- model_2 %>%
broom::augment()
fitted_points_2
# 2.c) Extract model summary info
model_2 %>%
broom::glance()
# 3. Make predictions on test data. Compare this to use of broom::augment()
# for fitted_points()
predicted_points_2 <- model_2 %>%
broom::augment(newdata = fake_test)
predicted_points_2
# Save predictions in pretend_test data frame
fake_test <- fake_test %>%
mutate(
log10_SalePrice_hat = predicted_points_2$.fitted,
SalePrice_hat = 10^log10_SalePrice_hat
)
# HAVE TO LOG BEFORE FITTING AND UNLOG AFTER PREDICTING
# Chunk 12
MLmetrics::RMSLE(y_pred = fake_test$SalePrice_hat, y_true = fake_test$SalePrice)
# Chunk 13
submission <- sample_submission %>%
mutate(SalePrice = mean(training$SalePrice))
write_csv(submission, path = "data/submission_due_diligence.csv")
# Chunk 14
# Chunk 15
# Chunk 16
submission <- sample_submission %>%
mutate(SalePrice = mean(training$SalePrice))
write_csv(submission, path = "data/submission_reach_for_stars.csv")
# Chunk 17
# Chunk 18
# Chunk 19
submission <- sample_submission %>%
mutate(SalePrice = mean(training$SalePrice))
write_csv(submission, path = "data/submission_diminishing_returns.csv")
View(fake_test)
tally(fake_train$HalfBath)
# Chunk 1: setup
# Load all your packages here:
library(tidyverse)
library(scales)
library(Mlmetrics)
library(broom)
# Set default behavior for all code chunks here:
knitr::opts_chunk$set(
echo = TRUE, warning = FALSE, message = FALSE,
fig.width = 16/2, fig.height = 9/2
)
# Set seed value of random number generator here. This is in order to get
# "replicable" randomness, so that any results based on random sampling or
# resampling are replicable everytime you knit this file. Why use a seed value
# of 76? For no other reason than 76 is one of my favorite numbers:
# https://www.youtube.com/watch?v=xjJ7FheCkCU
set.seed(76)
# Chunk 2
training <- read_csv("data/train.csv")
test <- read_csv("data/test.csv")
sample_submission <- read_csv("data/sample_submission.csv")
# Chunk 4
glimpse(sample_submission)
# Chunk 5
#variables:   Categorical - BldgType,  Numerical - GrLivArea
# training <- training %>%
#   select(Id, GrLivArea, BldgType, SalePrice)
# test <- test %>%
#   select(Id, GrLivArea, BldgType)
# Chunk 6
fake_train <- training %>%
sample_frac(0.75)
fake_test <- training %>%
anti_join(fake_train, by="Id")
# Chunk 7
# 1. Fit model to training data
model_1_formula <- as.formula("SalePrice~ GrLivArea + BldgType")
model_1 <- lm(model_1_formula, data = fake_train)
# 2.a) Extract regression table with confidence intervals
model_1 %>%
broom::tidy(conf.int = TRUE)
# 2.b) Extract point-by-point info of points used to fit model
fitted_points_1 <- model_1 %>%
broom::augment()
fitted_points_1
# 2.c) Extract model summary info
model_1 %>%
broom::glance()
# 3. Make predictions on test data. Compare this to use of broom::augment()
# for fitted_points()
predicted_points_1 <- model_1 %>%
broom::augment(newdata = fake_test)
predicted_points_1
# Chunk 8
MLmetrics::RMSLE(y_pred = predicted_points_1$SalePrice ,y_true =  predicted_points_1$.fitted)
# Chunk 9
submission <- sample_submission %>%
mutate(SalePrice = mean(training$SalePrice))
write_csv(submission, path = "data/submission_mvp.csv")
# Chunk 10
fake_train <- fake_train %>%
mutate(log10_SalePrice = log10(SalePrice),
log10_GrLivArea = log10(GrLivArea),
log10_OverallQual = log10(OverallQual),
)
fake_test <- fake_test %>%
mutate(log10_SalePrice = log10(SalePrice),
log10_GrLivArea = log10(GrLivArea),
log10_OverallQual = log10(OverallQual),
)
# Chunk 11
# 1. Fit model to training data
#model_2_formula <- as.formula("SalePrice~ GrLivArea + BldgType + OverallQual + HalfBath + Foundation + SaleCondition")
model_2_formula <- as.formula("log10_SalePrice~ log10_GrLivArea + BldgType + log10_OverallQual + HalfBath + Foundation + SaleCondition")
model_2 <- lm(model_2_formula, data = fake_train)
# 2.a) Extract regression table with confidence intervals
model_2 %>%
broom::tidy(conf.int = TRUE)
# 2.b) Extract point-by-point info of points used to fit model
fitted_points_2 <- model_2 %>%
broom::augment()
fitted_points_2
# 2.c) Extract model summary info
model_2 %>%
broom::glance()
# 3. Make predictions on test data. Compare this to use of broom::augment()
# for fitted_points()
predicted_points_2 <- model_2 %>%
broom::augment(newdata = fake_test)
predicted_points_2
# Save predictions in pretend_test data frame
fake_test <- fake_test %>%
mutate(
log10_SalePrice_hat = predicted_points_2$.fitted,
SalePrice_hat = 10^log10_SalePrice_hat
)
# HAVE TO LOG BEFORE FITTING AND UNLOG AFTER PREDICTING
# Chunk 12
MLmetrics::RMSLE(y_pred = fake_test$SalePrice_hat, y_true = fake_test$SalePrice)
# Chunk 13
submission <- sample_submission %>%
mutate(SalePrice = mean(training$SalePrice))
write_csv(submission, path = "data/submission_due_diligence.csv")
# Chunk 14
# Chunk 15
# Chunk 16
submission <- sample_submission %>%
mutate(SalePrice = mean(training$SalePrice))
write_csv(submission, path = "data/submission_reach_for_stars.csv")
# Chunk 17
# Chunk 18
# Chunk 19
submission <- sample_submission %>%
mutate(SalePrice = mean(training$SalePrice))
write_csv(submission, path = "data/submission_diminishing_returns.csv")
fake_train <- fake_train %>%
mutate(log10_SalePrice = log10(SalePrice),
log10_GrLivArea = log10(GrLivArea),
log10_OverallQual = log10(OverallQual)
)
fake_test <- fake_test %>%
mutate(log10_SalePrice = log10(SalePrice),
log10_GrLivArea = log10(GrLivArea),
log10_OverallQual = log10(OverallQual)
)
# 1. Fit model to training data
#model_2_formula <- as.formula("SalePrice~ GrLivArea + BldgType + OverallQual + HalfBath + Foundation + SaleCondition")
model_2_formula <- as.formula("log10_SalePrice~ log10_GrLivArea + BldgType + log10_OverallQual + HalfBath + Foundation + SaleCondition")
model_2 <- lm(model_2_formula, data = fake_train)
# 2.a) Extract regression table with confidence intervals
model_2 %>%
broom::tidy(conf.int = TRUE)
# 2.b) Extract point-by-point info of points used to fit model
fitted_points_2 <- model_2 %>%
broom::augment()
fitted_points_2
# 2.c) Extract model summary info
model_2 %>%
broom::glance()
# 3. Make predictions on test data. Compare this to use of broom::augment()
# for fitted_points()
predicted_points_2 <- model_2 %>%
broom::augment(newdata = fake_test)
predicted_points_2
# Save predictions in pretend_test data frame
fake_test <- fake_test %>%
mutate(
log10_SalePrice_hat = predicted_points_2$.fitted,
SalePrice_hat = 10^log10_SalePrice_hat
)
# HAVE TO LOG BEFORE FITTING AND UNLOG AFTER PREDICTING
MLmetrics::RMSLE(y_pred = fake_test$SalePrice_hat, y_true = fake_test$SalePrice)
MLmetrics::RMSLE(y_pred = predicted_points_1$SalePrice ,y_true =  predicted_points_1$.fitted)
