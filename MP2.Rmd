---
title: "SDS/CSC 293 Mini-Project 2: Multiple Regression"
author: "Group XX: Heloise Cheruvalath, Karen Santamaria"
date: "Wednesday, March 6^th^, 2019"
output:
  html_document:
    highlight: tango
    theme: cosmo
    toc: yes
    toc_depth: 2
    toc_float:
      collapsed: true
    df_print: kable
---

```{r setup, include=FALSE}
# Load all your packages here:
library(tidyverse)
library(scales)
library(Metrics)
library(broom)

# Set default behavior for all code chunks here:
knitr::opts_chunk$set(
  echo = TRUE, warning = FALSE, message = FALSE,
  fig.width = 16/2, fig.height = 9/2
)

# Set seed value of random number generator here. This is in order to get
# "replicable" randomness, so that any results based on random sampling or
# resampling are replicable everytime you knit this file. Why use a seed value
# of 76? For no other reason than 76 is one of my favorite numbers:
# https://www.youtube.com/watch?v=xjJ7FheCkCU
set.seed(76)
```

You will be submiting an entry to Kaggle's [House Prices: Advanced Regression Techniques](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/){target="_blank"} by fitting a **multiple regression** model $\hat{f}(x)$.



***



# EDA

Read in data provided by Kaggle for this competition. They are organized in the `data/` folder of this RStudio project:

```{r}
training <- read_csv("data/train.csv")
test <- read_csv("data/test.csv")
sample_submission <- read_csv("data/sample_submission.csv")
```

Before performing any model fitting, you should always conduct an exploratory data analysis. This will help guide and inform your model fitting. 

## Look at your data!

Always, ALWAYS, **ALWAYS** start by looking at your raw data. This gives you visual sense of what information you have to help build your predictive models. To get a full description of each variable, read the data dictionary in the `data_description.txt` file in the `data/` folder.

Note that the following code chunk has `eval = FALSE` meaning "don't evaluate this chunk with knitting" because `.Rmd` files won't knit if they include a `View()`:

```{r, eval = FALSE}
View(training)
glimpse(training)

View(test)
glimpse(test)


fake_train <- training %>%
  sample_frac(0.75)
fake_test <- training %>%
  anti_join(fake_train, by="Id")

```

In particular, pay close attention to the variables and variable types in the
`sample_submission.csv`. Your submission must match this exactly.

```{r}
glimpse(sample_submission)
```

## Data wrangling

As much as possible, try to do all your data wrangling here:

```{r}

#variables:   Categorical - BldgType,  Numerical - GrLivArea

training <- training %>% 
  select(Id, GrLivArea, BldgType, SalePrice)
test <- test %>% 
  select(Id, GrLivArea, BldgType)
```



***



# Minimally viable product

## Model fitting

```{r}
# 1. Fit model to training data
model_1_formula <- as.formula("SalePrice~ GrLivArea + BldgType")
model_1 <- lm(model_1_formula, data = fake_train)

# 2.a) Extract regression table with confidence intervals
model_1 %>%
  broom::tidy(conf.int = TRUE)

# 2.b) Extract point-by-point info of points used to fit model
fitted_points_1 <- model_1 %>%
  broom::augment()
fitted_points_1

# 2.c) Extract model summary info
model_1 %>%
  broom::glance()

# 3. Make predictions on test data. Compare this to use of broom::augment()
# for fitted_points()
predicted_points_1 <- model_1 %>%
  broom::augment(newdata = fake_test)
predicted_points_1



```


## Estimate of your Kaggle score

```{r}
rmsle(predicted_points_1$.fitted , predicted_points_1$SalePrice)
```


## Create your submission CSV

```{r}
submission <- sample_submission %>% 
  mutate(SalePrice = mean(training$SalePrice))

write_csv(submission, path = "data/submission_mvp.csv")
```


## Screenshot of your Kaggle score

Our score based on our submission's "Root Mean Squared Logarithmic Error" was 0.42918.

![](score_screenshot.png){ width=100% }


## Comparisons of estimated scores and Kaggle scores





***





# Due diligence

## Model fitting

```{r}

```


## Estimate of your Kaggle score

```{r}

```


## Create your submission CSV

```{r}
submission <- sample_submission %>% 
  mutate(SalePrice = mean(training$SalePrice))

write_csv(submission, path = "data/submission_due_diligence.csv")
```


## Screenshot of your Kaggle score

Our score based on our submission's "Root Mean Squared Logarithmic Error" was 0.42918.

![](score_screenshot.png){ width=100% }


## Comparisons of estimated scores and Kaggle scores





***





# Reaching for the stars

## Model fitting

```{r}

```


## Estimate of your Kaggle score

```{r}

```


## Create your submission CSV

```{r}
submission <- sample_submission %>% 
  mutate(SalePrice = mean(training$SalePrice))

write_csv(submission, path = "data/submission_reach_for_stars.csv")
```


## Screenshot of your Kaggle score

Our score based on our submission's "Root Mean Squared Logarithmic Error" was 0.42918.

![](score_screenshot.png){ width=100% }


## Comparisons of estimated scores and Kaggle scores






***





# Point of diminishing returns

## Model fitting

```{r}

```


## Estimate of your Kaggle score

```{r}

```


## Create your submission CSV

```{r}
submission <- sample_submission %>% 
  mutate(SalePrice = mean(training$SalePrice))

write_csv(submission, path = "data/submission_diminishing_returns.csv")
```


## Screenshot of your Kaggle score

Our score based on our submission's "Root Mean Squared Logarithmic Error" was 0.42918.

![](score_screenshot.png){ width=100% }


## Comparisons of estimated scores and Kaggle scores
